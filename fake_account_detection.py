# # -*- coding: utf-8 -*-
# """Fake_Account_Detection.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1-eR5ZFLldS6gblyJ-XGQ6D1b3mRtkVMg
# """

# # pip install tensorflow

# #importing all the required libraries
# import pandas as pd
# import matplotlib.pyplot as plt
# import numpy as np
# import seaborn as sns

# import tensorflow as tf
# from tensorflow import keras
# from tensorflow.keras.layers import Dense, Activation, Dropout
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.metrics import Accuracy

# from sklearn import metrics
# from sklearn.preprocessing import LabelEncoder
# from sklearn.metrics import classification_report,accuracy_score,roc_curve,confusion_matrix

# #styling purpose
# # pip install jupyterthemes

# from jupyterthemes import jtplot
# jtplot.style(theme = 'monokai', context = 'notebook', ticks = True, grid = False)

# #Load the training and testing datasets
# instagram_df_test = pd.read_csv('test.csv')
# instagram_df_train = pd.read_csv('train.csv')

# #the dataset which will be used for training purpose (576 records)
# instagram_df_train

# #the dataset which will be used for testing purpose (120 records)
# instagram_df_test

# """*   **<h3><i>Performing Exploratory Data Anlaysis For "Training Dataset"**"""

# #Getting dataframe info

# instagram_df_train.info()

# #Statistical summary of the dataframe
# instagram_df_train.describe()

# #Check if null values exist
# instagram_df_train.isnull().sum()

# #Number of unique values in the profile pic column
# instagram_df_train['profile pic'].value_counts()

# #Number of fake and real accounts
# instagram_df_train['fake'].value_counts()

# #Number of accounts having an external URL
# instagram_df_train['external URL'].value_counts()

# #Number of accounts having description length over 50
# (instagram_df_train['description length'] > 50).sum()

# """

# *   **<h2><i>Performing Exploratory Data Anlaysis For "Testing Dataset"**

# """

# instagram_df_test.info()

# instagram_df_test.describe()

# instagram_df_test.isnull().sum()

# instagram_df_test['fake'].value_counts()

# """**<H1>TASK 3**

# ---


# **<h2>PERFROMING DATA VISUALIZATION**
# """

# #Vislualizing the number of fake and real accounts (using seaborn library)
# # sns.countplot(instagram_df_train['fake'])

# # #Visualizing the private column
# # sns.countplot(instagram_df_train['private'],palette = "PuBu")

# # #Visualizing the profile pic feature
# # sns.countplot(instagram_df_train['profile pic'],palette = "Pastel2")

# # #Visualizing the length of usernames(Histogram)
# # plt.figure(figsize = (20, 10))
# # sns.distplot(instagram_df_train['nums/length username'],kde=True)

# # #Correlation heatmap
# # plt.figure(figsize=(15,15))
# # cm = instagram_df_train.corr()
# # ax = plt.subplot()
# # sns.heatmap(cm, annot = True, ax = ax)

# # sns.countplot(instagram_df_test['fake'])

# # sns.countplot(instagram_df_test['private'],palette = "Set2")

# # sns.countplot(instagram_df_test['profile pic'])

# """**<H1>TASK 4**

# ---

# **<h2>PREPARING THE DATA TO FEED THE MODEL**
# """

# #Preparing inputs for the model (Dropping the fake column from both training and testing dataset)
# x_train = instagram_df_train.drop(columns = ['fake'])
# x_test = instagram_df_test.drop(columns = ['fake'])
# x_train

# x_test

# #Preparing the outputs (Takin only the fake column into consideration.)
# y_train = instagram_df_train['fake']
# y_test = instagram_df_test['fake']
# y_train

# y_test

# #Scaling the data before training the model (Normalize the data)
# from sklearn.preprocessing import StandardScaler, MinMaxScaler

# scaler_x = StandardScaler()
# X_train = scaler_x.fit_transform(x_train)
# X_test = scaler_x.transform(x_test)

# Y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
# Y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)

# #Having a look at the shapes after scaling the data
# X_train.shape,X_test.shape

# Y_train.shape,Y_test.shape

# #Percentage of Traininf data
# Training_data_percentage = len(X_train)/(len(X_train) + len(X_test) ) * 100
# Training_data_percentage

# Testing_data_percentage = len(X_test)/(len(X_train) + len(X_test) ) * 100
# Testing_data_percentage

# """**<H1>TASK-6**

# ---

# **<h2><I>BUILD A SIMPLE DEEP LEARNING MODEL**

# """

# import tensorflow.keras
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Dropout
# import pickle 

# #Building the main model***

# model = Sequential()
# model.add(Dense(50,input_dim = 11, activation = "relu")) #Initial Layer
# model.add(Dropout(0.3))
# model.add(Dense(150, activation = "relu"))
# model.add(Dropout(0.3))
# model.add(Dense(25, activation = "relu"))
# model.add(Dropout(0.3))
# model.add(Dense(2, activation = "softmax")) #output layer

# model.summary()

# model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# epochs_hist = model.fit(X_train, Y_train, epochs = 20, verbose = 1, validation_split = 0.1)
# #pickle.dump(model,open("fad.pkl","wb"))


# # Save the model using the SavedModel format
# model.save("fad_model.h5")

# # Load the model
# loaded_model = keras.models.load_model("fad_model.h5")

# """**<h2> TASK-7</h2>**

# ---


# **<h2><i>ASSESSING THE PRERFORMANCE OF THE MODEL**
# """

# print(epochs_hist.history.keys())

# plt.plot(epochs_hist.history['loss'])
# plt.plot(epochs_hist.history['val_loss'])

# plt.title('Model Loss Progressioin During Training/Validation')
# plt.xlabel('Epoch Number')
# plt.ylabel('Training and Validation Losses')
# plt.legend(['Training Loss','Valdiation Loss'])

# predicted = model.predict(X_test)

# predicted_value = []
# test = []
# for i in predicted:
#     predicted_value.append(np.argmax(i))

# for i in Y_test:
#     test.append(np.argmax(i))

# print(classification_report(test, predicted_value))

# plt.figure(figsize=(10, 10))
# con_matrix = confusion_matrix(test,predicted_value)
# sns.heatmap(con_matrix, annot=True)





#########new code


# -*- coding: utf-8 -*-
"""Fake Instagram Profile Detection Model ....

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ljaM969ZpETrPZw4B8K5sjYnWU85M3c8
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import Accuracy

from sklearn import metrics
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report,accuracy_score,roc_curve,confusion_matrix

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Load the training dataset
instagram_df_train=pd.read_csv('train.csv')
instagram_df_train

# Load the testing data
instagram_df_test=pd.read_csv('test.csv')
instagram_df_test

"""# Statistical Analysis"""

instagram_df_train.head()

instagram_df_train.tail()

# Getting dataframe info
instagram_df_train.info()

# Get the statistical summary of the dataframe
instagram_df_train.describe()

# Checking if null values exist
instagram_df_train.isnull().sum()

# Get the number of unique values in the "profile pic" feature
instagram_df_train['profile pic'].value_counts()

# Get the number of unique values in "fake" (Target column)
instagram_df_train['fake'].value_counts()

"""# Data Visualization"""

# Visualize the data
sns.countplot(instagram_df_train['fake'])
plt.show()

# Visualize the private column data
sns.countplot(instagram_df_train['private'])
plt.show()

# Visualize the "profile pic" column data
sns.countplot(instagram_df_train['profile pic'])
plt.show()

# Visualize the data
# plt.figure(figsize = (20, 10))
# sns.distplot(instagram_df_train['nums/length username'])
# plt.show()

# # Correlation plot
# plt.figure(figsize=(20, 20))
# cm = instagram_df_train.corr()
# ax = plt.subplot()
# sns.heatmap(cm, annot = True, ax = ax)
# plt.show()

"""# Data Modelling"""

# Training and testing dataset (inputs)
X_train = instagram_df_train.drop(columns = ['fake'])
X_test = instagram_df_test.drop(columns = ['fake'])
X_train

# Training and testing dataset (Outputs)
y_train = instagram_df_train['fake']
y_test = instagram_df_test['fake']
y_train

# Scale the data before training the model
from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler_x = StandardScaler()
X_train = scaler_x.fit_transform(X_train)
X_test = scaler_x.transform(X_test)

y_train = tf.keras.utils.to_categorical(y_train, num_classes = 2)
y_test = tf.keras.utils.to_categorical(y_test, num_classes = 2)

y_train

import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential()
model.add(Dense(50, input_dim=11, activation='relu'))
model.add(Dense(150, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(150, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(25, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(2,activation='softmax'))

model.summary()

model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

epochs_hist = model.fit(X_train, y_train, epochs = 50,  verbose = 1, validation_split = 0.1)


# Save the model using the SavedModel format
model.save("fad_model1.h5")

# Load the model
loaded_model = keras.models.load_model("fad_model1.h5")

"""# Model Validation and Results"""

print(epochs_hist.history.keys())

plt.plot(epochs_hist.history['loss'])
plt.plot(epochs_hist.history['val_loss'])

plt.title('Model Loss Progression During Training/Validation')
plt.ylabel('Training and Validation Losses')
plt.xlabel('Epoch Number')
plt.legend(['Training Loss', 'Validation Loss'])
plt.show()

predicted = model.predict(X_test)

predicted_value = []
test = []
for i in predicted:
    predicted_value.append(np.argmax(i))

for i in y_test:
    test.append(np.argmax(i))

print(classification_report(test, predicted_value))

plt.figure(figsize=(10, 10))
cm=confusion_matrix(test, predicted_value)
sns.heatmap(cm, annot=True)
plt.show()